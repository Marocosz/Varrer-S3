# ================= IMPORTS E BIBLIOTECAS =================

# 'boto3': O SDK oficial da AWS. √â a ponte entre o Python e a nuvem.
import boto3

# 'os': Permite interagir com o sistema operacional (ler vari√°veis de ambiente, manipular caminhos).
import os

# 'defaultdict': Uma vers√£o inteligente de dicion√°rio que cria valores padr√£o se a chave n√£o existir.
# √ötil para n√£o precisarmos ficar verificando "se a chave existe" antes de somar +1.
from collections import defaultdict

# 'datetime': Usado para pegar a data/hora atual para colocar no cabe√ßalho do relat√≥rio.
from datetime import datetime

# 'tqdm': Cria aquela barra de progresso visual no terminal para voc√™ saber que o script n√£o travou.
from tqdm import tqdm

# 'dotenv': Biblioteca externa que l√™ o arquivo '.env' e carrega as vari√°veis para o sistema.
from dotenv import load_dotenv

# ================= CARREGAMENTO DE AMBIENTE =================

# Esta linha procura um arquivo chamado '.env' na mesma pasta do script.
# Ela l√™ o conte√∫do e coloca na mem√≥ria como se fossem vari√°veis do sistema.
# Isso garante seguran√ßa: suas senhas n√£o ficam escritas no c√≥digo.
load_dotenv()

# Pegamos o nome do bucket. Se n√£o estiver no .env, retornar√° None.
BUCKET_NAME = os.getenv('BUCKET_NAME')

# --- NOVO ---
# Carrega a pasta alvo definida no .env.
# O segundo par√¢metro ('') √© um valor padr√£o: se a vari√°vel TARGET_FOLDER n√£o existir no .env,
# assumimos que √© uma string vazia. Na AWS, string vazia no prefixo significa "Bucket Inteiro".
TARGET_FOLDER = os.getenv('TARGET_FOLDER', '')

OUTPUT_FILE = 'relatorio_s3.md'

# ================= FUN√á√ïES DO SISTEMA =================

def get_s3_client():
    """
    Cria e retorna o cliente de conex√£o com o S3.
    """
    # Buscamos as credenciais carregadas do arquivo .env
    aws_access_key = os.getenv('AWS_ACCESS_KEY_ID')
    aws_secret_key = os.getenv('AWS_SECRET_ACCESS_KEY')
    aws_region = os.getenv('AWS_REGION')

    # Valida√ß√£o de Seguran√ßa: Se as chaves n√£o existirem, paramos o script agora.
    if not aws_access_key or not aws_secret_key:
        raise ValueError("ERRO: Credenciais AWS n√£o encontradas. Verifique seu arquivo .env")

    # Criamos o cliente 's3'. 
    # Diferente de usar 'Session', aqui passamos as chaves explicitamente.
    # Isso garante que o boto3 use O QUE EST√Å NO .ENV, ignorando qualquer configura√ß√£o global do PC.
    return boto3.client(
        's3',
        aws_access_key_id=aws_access_key,
        aws_secret_access_key=aws_secret_key,
        region_name=aws_region
    )

def scan_bucket(bucket_name, prefix_folder):
    """
    Varre o bucket filtrando por uma pasta espec√≠fica (Prefix).
    """
    
    # Valida√ß√£o inicial
    if not bucket_name:
        print("ERRO CR√çTICO: BUCKET_NAME n√£o definido no arquivo .env")
        return {}, set(), set(), 0

    # Obt√©m a conex√£o autenticada
    s3 = get_s3_client()
    
    # --- CONCEITO IMPORTANTE: PAGINA√á√ÉO ---
    # O endpoint 'list_objects_v2' da AWS retorna no m√°ximo 1.000 arquivos por vez.
    # O 'paginator' automatiza o processo de pedir a p√°gina 1, depois a 2, depois a 3...
    paginator = s3.get_paginator('list_objects_v2')
    
    # Estrutura de dados para contagem:
    # { 'caminho/da/pasta': { 2021: 10 arquivos, 2022: 5 arquivos } }
    folder_stats = defaultdict(lambda: defaultdict(int))
    
    # Sets (conjuntos) para armazenar caminhos √∫nicos e evitar duplicatas
    all_known_paths = set()    # Guarda TUDO que parece ser uma pasta
    files_found_paths = set()  # Guarda apenas pastas que T√äM arquivos dentro

    # L√≥gica apenas para mostrar uma mensagem bonita no terminal
    start_msg = f"bucket '{bucket_name}'"
    if prefix_folder:
        start_msg += f" na pasta '{prefix_folder}'"
    else:
        start_msg += " (RAIZ TOTAL)"

    print(f"Iniciando conex√£o com a AWS e varredura em: {start_msg}...")
    
    # --- PONTO CR√çTICO: FILTRAGEM POR PREFIXO ---
    # Aqui passamos o argumento 'Prefix'. Isso √© crucial para performance.
    # Ao passar o prefixo, a filtragem acontece NOS SERVIDORES DA AWS.
    # O seu script nem chega a receber informa√ß√µes de arquivos fora dessa pasta.
    # Isso economiza banda de internet, processamento local e tempo.
    page_iterator = paginator.paginate(
        Bucket=bucket_name, 
        Prefix=prefix_folder  # Se for vazio, traz tudo. Se tiver texto, filtra.
    )
    
    total_files = 0

    # Loop principal: Itera sobre cada p√°gina de 1000 objetos retornada pela AWS
    for page in tqdm(page_iterator, desc="Processando objetos"):
        
        # Se o bucket estiver vazio ou a p√°gina n√£o tiver conte√∫do, pulamos.
        if 'Contents' not in page:
            continue

        # Loop interno: Itera sobre cada arquivo dentro da p√°gina atual
        for obj in page['Contents']:
            key = obj['Key']             # O caminho completo (ex: "planilhas/2023/jan.xlsx")
            last_modified = obj['LastModified'] # Data da √∫ltima edi√ß√£o
            
            # --- TRATAMENTO DE PASTAS VIRTUAIS ---
            # O S3 n√£o tem pastas reais. √Äs vezes, softwares de FTP criam objetos vazios terminados em '/'
            # para simular uma pasta. Se encontrarmos um desses, guardamos o caminho e pulamos.
            if key.endswith('/'):
                all_known_paths.add(key)
                continue

            # --- EXTRA√á√ÉO DE DIRET√ìRIO ---
            # 'os.path.dirname' pega "a/b/c.txt" e retorna "a/b"
            folder_path = os.path.dirname(key)
            
            # Se o arquivo estiver na raiz do bucket, o dirname volta vazio. Chamamos de "Raiz".
            if not folder_path:
                folder_path = "Raiz"
            
            # Marcamos: "Esta pasta cont√©m arquivos reais"
            files_found_paths.add(folder_path)
            
            # --- RECONSTRU√á√ÉO DE HIERARQUIA ---
            # Se temos o arquivo em "a/b/c", precisamos garantir que o relat√≥rio saiba
            # que a pasta "a" existe e a pasta "a/b" existe, mesmo que n√£o tenham arquivos diretos.
            parts = folder_path.split('/')
            current_build = ""
            for part in parts:
                if part == "Raiz": continue
                # Reconstr√≥i o caminho passo a passo: "a/", depois "a/b/"
                current_build = f"{current_build}{part}/" if current_build else f"{part}/"
                all_known_paths.add(current_build)

            # --- ESTAT√çSTICA ---
            # Extrai apenas o ano (ex: 2022) do objeto datetime
            year = last_modified.year
            
            # Soma +1 na contagem daquela pasta, naquele ano
            folder_stats[folder_path][year] += 1
            total_files += 1

    return folder_stats, all_known_paths, files_found_paths, total_files

def generate_markdown_report(folder_stats, all_known_paths, files_found_paths):
    """
    Gera o arquivo f√≠sico (.md) com os dados organizados.
    """
    print(f"\nEscrevendo relat√≥rio em {OUTPUT_FILE}...")
    
    # Une os dois conjuntos de caminhos e ordena alfabeticamente
    sorted_folders = sorted(list(all_known_paths | files_found_paths))
    
    # Abre o arquivo para escrita ('w'). O encoding='utf-8' √© vital para n√£o quebrar acentos.
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        
        # Cabe√ßalho do Markdown
        f.write(f"# Relat√≥rio de Auditoria S3\n")
        f.write(f"**Data:** {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\n")
        f.write(f"**Bucket:** `{BUCKET_NAME}`\n")
        
        # Mostra qual filtro foi usado no relat√≥rio final para evitar confus√£o
        filter_used = TARGET_FOLDER if TARGET_FOLDER else "(Raiz Total)"
        f.write(f"**Filtro (Prefix):** `{filter_used}`\n")
        f.write("---\n\n")

        # Itera sobre cada pasta identificada
        for folder in sorted_folders:
            # Limpeza visual: remove a barra final para o t√≠tulo ficar bonito
            search_key = folder.rstrip('/')
            if search_key == "": search_key = "Raiz"
            
            # T√≠tulo da se√ß√£o da pasta
            f.write(f"### üìÇ `{search_key}`\n")

            # CASO 1: A pasta tem arquivos nela (est√° no dicion√°rio folder_stats)
            if search_key in folder_stats:
                years_data = folder_stats[search_key]
                sorted_years = sorted(years_data.keys()) # Ordena anos (2020, 2021...)
                
                # Tabela Markdown
                f.write("| Ano | Qtd. Arquivos |\n")
                f.write("| :--- | :--- |\n")
                
                total_local = 0
                for year in sorted_years:
                    count = years_data[year]
                    total_local += count
                    f.write(f"| {year} | {count} |\n")
                
                f.write(f"\n**Total nesta pasta:** {total_local} arquivos\n")
            
            # CASO 2: A pasta existe na hierarquia, mas n√£o tem arquivos diretos (s√≥ subpastas)
            elif folder in all_known_paths and search_key not in files_found_paths:
                 f.write("> *‚ÑπÔ∏è Esta pasta cont√©m apenas subpastas.*\n")
            
            # CASO 3: Residual (Pasta vazia ou marcador)
            else:
                 f.write("> *Pasta vazia.*\n")

            # Separador visual entre se√ß√µes
            f.write("\n---\n")

    print("Conclu√≠do!")

# ================= EXECU√á√ÉO PRINCIPAL =================

if __name__ == "__main__":
    # Verifica se a varredura pode come√ßar
    if not BUCKET_NAME:
        print("‚ùå ERRO: Configure o nome do bucket no arquivo .env antes de rodar.")
    else:
        # 1. Coleta dados
        # Passamos o BUCKET_NAME e agora tamb√©m o TARGET_FOLDER (que pode ser vazio ou uma pasta)
        stats, all_paths, file_paths, total = scan_bucket(BUCKET_NAME, TARGET_FOLDER)
        
        print(f"Varredura completa. {total} arquivos encontrados.")
        
        # 2. Gera relat√≥rio
        generate_markdown_report(stats, all_paths, file_paths)